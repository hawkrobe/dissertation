%!TEX root = ../dissertation.tex
\begin{savequote}[125mm]
    The speaker wants to be understood. %, so he intends to speak in such a way that he will be interpreted in a certain way. 
    In order to judge how he will be interpreted, he uses his [...] starting theory of interpretation. % interpreter’s readiness to interpret along certain lines. %Central to this picture is what the speaker believes is 
%The speaker does not necessarily speak in such a way as to prompt the interpreter to apply this prior theory; he may deliberately dispose the interpreter to modify his prior theory. But the speaker’s view of the interpreter’s prior theory is not irrelevant to what he says, nor to what he means by his words; it is an important part of what he has to go on if he wants to be understood.
As speaker and interpreter talk, their ``prior'' theories become more alike; so do their ``passing'' theories. %The asymptote of agreement and understanding is when passing theories coincide. 
%But the passing theory cannot in general correspond to an interpreter’s linguistic competence. 
Not only does it have its changing list of proper names and gerrymandered vocabulary, but it includes every successful use of any other word or phrase, no matter how far out of the ordinary. 
Every deviation from ordinary usage, as long as it is agreed on for the moment (knowingly deviant, or not, on one, or both, sides), is in the passing theory as a feature of what the words mean on that occasion. 
Such meanings, transient though they may be, are literal.
\qauthor{\vspace{-2em}Donald Davidson, 1986}
\end{savequote}

\chapter{An inferential model of convention-formation}
\graphicspath{{./figures/modeling/}}

In this chapter, we present a computational model formalizing our theoretical account of how speakers coordinate on meaning under uncertainty. 
Our formal approach is grounded in the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena---including scalar implicature \cite{GoodmanStuhlmuller13_KnowledgeImplicature}, adjectival vagueness \cite{LassiterGoodman15_AdjectivalVagueness}, overinformativeness \cite{degen2019redundancy}, indirect questions \cite{hawkins_why_2015}, and other non-literal language use \cite{KaoWuBergenGoodman14_NonliteralNumberWords}---as arising from a process of recursive social reasoning.% \cite{GoodmanFrank16_RSATiCS}. 
%Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a puzzle of speaker production. 

At the core of any model of referential communication is the notion of a \emph{semantics} giving the meanings of utterances in the language. 
For the minimal examples in this chapter, it suffices to define a lexical function $\mathcal{L}: (w, o) \rightarrow \mathbb{R}$, assigning any word-object pair a real-valued meaning according to how well the word $w$ applies to the object $o$. 
This is a continuous generalization of classic truth-conditional semantics \shortcite{GrafEtAl16_BasicLevel}, where utterances may apply more or less well to different referents. 
For instance, the utterance \emph{dancer} may initially be expected to apply to a photorealistic image of a \texttt{ballerina} ($\mathcal{L}(\textit{dancer}, \texttt{ballerina}) = 0.99$) more than an abstract sketch of one ($\mathcal{L}(\textit{dancer}, \texttt{ballerina drawing}) =0.6$), but apply to both better than a non-category member like an image of dog falling down the stairs ($\mathcal{L}(\textit{dancer}, \texttt{dog}) = 0.05$).

In this framework, an $n$th order pragmatic speaker trying to refer to particular object $o \in \mathcal{O}$ assuming lexicon $\mathcal{L}$ selects an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length:
$$S_n(u | o, \mathcal{L}) \propto \exp{\left(\alpha \log L_{n-1}(o | u, \mathcal{L}) - \textrm{cost}(u)\right)}$$
where $\alpha$ is a soft-max optimality parameter controlling the extent to which the speaker maximizes over listener informativity. 
The listener, in turn, inverts the speaker model to reason about what underlying object $o$ the speaker is trying to convey, given their utterance $u$:
$$L_n(o | u, \mathcal{L}) \propto S_{n}(u | o, \mathcal{L})P(o)$$
\indent This recursion eventually bottoms out in a \emph{literal listener} who directly looks up the meaning of the utterance in the lexicon:
$$L_0(o | u, \mathcal{L}) \propto \mathcal{L}(u, o)\cdot P(o)$$

\section{Adapting to a single partner}
Now, we extend the lexicon from a lookup table or a static logical form into a dynamic, parameterized representation that can be constantly being updated.
To formalize this notion of semantic meaning, we begin with the additional assumption of \emph{lexical uncertainty} \cite{SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS,BergenLevyGoodman16_LexicalUncertainty}. 
That is, instead of assuming agents have fixed knowledge of $\mathcal{L}$, we allow for uncertainty over the exact meanings of lexical items in the current context (e.g. it may be initially unclear what ``the dancer" might refer to). 
Concretely, we put a prior $P(\mathcal{L})$ over the identity of a partner's true lexicon, which may be initially biased toward certain meanings from previous experience. 

Bayesian updating then gives a rule for updating expectations about this true lexicon conditioned on repeated observations of a partner's behavior:
$$P_{L_n}(\mathcal{L} | d) \propto P(\mathcal{L})\prod_i S_n(o_i | u_i, \mathcal{L})$$
where $d = \{o_i, u_i\}$ is a set of observations containing utterances $u_i$ referring to objects $o_i$ from previous exchanges with that partner.
The listener marginalizes over this posterior when interpreting a new utterance $u$ from the speaker:
$$L_n(o | u, d) \propto \sum_\mathcal{L}P_{L_n}(\mathcal{L}|d)L_n(o | u,\mathcal{L})$$
The speaker, in turn, considers what utterances would be most informative for such a listener:
$$S_n(u | o, d) \propto \exp( \alpha\log\left(\sum_{\mathcal{L}} P_{S_n}(\mathcal{L} | d) L_{n-1}(o | u, \mathcal{L})\right) - \textrm{cost}(u) )$$
where the posterior over lexica $P_{S_n}(\mathcal{L} | d)$, uses the listener likelihood $L_{n-1}$ instead of the speaker likelihood:
$$P_{S_n}(\mathcal{L} | d) \propto P(\mathcal{L})\prod_i L_{n-1}(u_i | o_i, \mathcal{L})$$
For the purposes of this paper, we fix the depth of recursion at $n = 2$.
This model is implemented in the probabilistic programming language WebPPL \cite{GoodmanStuhlmuller14_DIPPL}. \footnote{These results can be reproduced running our code in the browser at \url{http://forestdb.org/models/conventions.html}}. 

\subsection{Model Results}

\begin{figure}
\centering
    \includegraphics[scale=.8]{modelResults.pdf}
  \caption{Schematic of model}
  \label{fig:modelResults}
\end{figure}

\paragraph{Coordination}
First, we show how agents updating their meaning functions in this way can coordinate even in the absence of strong initial priors. 
The initial choices in an interaction can be taken as evidence for a particular lexicon and become the basis for successful communication, even when both speaker and listener are uncertain at the beginning.
As a simple test case, consider an environment with two objects ($\{o_1, o_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) with equal production costs. 
For the prior $P(\mathcal{L})$ over the meaning of each utterance, we define a Beta distribution\footnote{In our implementation, we use exact enumeration over coarse-grained bins; experiments using variational inference on the full continuous distribution give similar results}, so on the first round both utterances are equally likely to apply to either shape. 
If the speaker were trying to get their partner to pick $o_1$, then because each utterance is equally (un)informative, they could only randomly sample one (say, $u_1$), and observe the listener's selection of a shape (say, $o_1$, a correct response). 
On the next round, the speaker uses the observed pair $\{u_1, o_1\}$ to update their beliefs about their partner's true lexicon, uses these beliefs to generate a new utterance, and so on. 
To examine expected dynamics over multiple rounds, we forward sample many possible trajectories.

We observe several important qualitative effects in our simulations. 
First, and more fundamentally, the evidence that a knowledgeable listener responded to utterance $u$ by choosing a particular object $o$ provides support for lexicons in which $u$ is a good fit for $o$. 
Hence, the likelihood of the speaker using $u$ to refer to $o$ will increase on subsequent rounds (see Fig.\ref{fig:modelResults}A). 
In other words, the initial symmetry between the meanings can be broken by initial random choices, leading to completely arbitrary but stable mappings in future rounds. 

Second, because the listener is updating their meaning representation from the same observations under the same set of assumptions, both partners converge on a \emph{shared} set of meanings; hence, the expected accuracy of selecting the target object rises on future rounds (see Fig. \ref{fig:modelResults}B). 
Third, because one's partner is assumed to be pragmatic via recursive Rational Speech Act mechanisms, agents can also learn about \emph{unheard} utterances. 
Observing $d = \{(u_1, o_1)\}$ also provides evidence that $u_2$ is \emph{not} a good fit for $o_1$.
This effect arises from Gricean maxims: if $u_2$ were a better fit for $o_1$, the speaker would have used it instead \cite{Grice75_LogicConversation}. 
Fourth, \emph{failed references} can lead to conventions just as effectively as successful references: if the speaker intends $o_1$ and says $u_1$, but then the listener incorrectly picks $o_2$, the speaker will take this as evidence that $u_1$ actually means $o_2$ in their partner's lexicon and become increasingly likely to use it that way on subsequent rounds.

\paragraph{Reduction}
Next, we show how our model explains reduction of utterance length over multiple interactions. 
For utterances to be reduced, of course, they must vary in length, so we extend our grammar to include \emph{conjunctions}. 
Conjunctions are one of the simplest ways to constructi longer utterances compositionally from lexical primitives, using the product rule:
$$\mathcal{L}(u_i \textrm{ and } u_j, o) = \mathcal{L}(u_i, o) \times \mathcal{L}(u_j, o)$$
\indent Analogous to the \emph{tangram} stimuli used in the reference game reviewed in Chapter 1, which have many ambiguous features and figurative perspectives that may be evoked in speaker descriptions, we consider a scenario the two objects $\{o_1, o_2\}$ differ along two different features. 
The speaker thus has four primitive words at their disposal -- two words for the first feature ($\{u_{11}, u_{12}\}$) and two for the second $\{u_{21}, u_{22}\}$. 

While we established in the previous section that conventions can emerge over a reference game in the complete absence of initial preferences, players often bring such preferences to the table. 
A player who hears `ice skater' on the first round of a tangrams task is more likely to select some objects more than others, even though they still have some uncertainty over its meaning in the context. 
To show that our model can accommodate this fact, we allow the speaker's initial prior meanings to be slightly biased. 
We assume $u_{11}$ and $u_{21}$ are a priori more likely to mean $o_1$ and $u_{12}$ and $u_{22}$ are more likely to mean $o_2$.

We ran 1000 forward samples of 6 rounds of speaker-listener interaction, and averaged over the utterance length at each round \footnote{In our simulations, we used $\alpha = 10$ and found the basic reduction effect over a range of different biases}. 
Our results are shown in Figure \ref{fig:modelResults}C: the expected utterance length decreases systematically over each round. 
To illustrate in more detail how this dynamic is driven by an initial rational preference for redundancy relaxing as reference becomes more reliable, we walk step-by-step through a single trajectory. 

Consider a speaker who wants to refer to object $o_1$. 
They believe their knowledgeable partner is slightly more likely to interpret their language using a lexicon in which $u_{11}$ and $u_{12}$ apply to this object, due to their initial bias. 
However, there is still a reasonable chance that one or the other alone actually refers more strongly to $o_2$ in the true lexicon. 
Thus, it is useful to produce the conjunction "$u_{11}$ and $u_{12}$" to hedge against this possibility, despite its higher cost. 
Upon observing the listener's response (say, $o_1$), the evidence is indeterminate about the separate meanings of $u_{11}$ and $u_{12}$ but both become increasingly likely to refer to $o_1$. 
In the trade-off between informativity and cost, the shorter utterances remain probable options. 
Once the speaker chooses one of them, the symmetry collapses and that utterance remains most probable in future rounds. 
In this way, meaningful sub-phrases are omitted over time as the speaker becomes more confident about the true lexicon. 

\section{Generalizing to new partners: introducing hierarchical structure}

\begin{figure}
\centering
    \includegraphics[scale=.45]{task1_model.pdf}
  \caption{Schematic of model}
  \label{fig:task1model}
\end{figure}

Next, we can straightforwardly extend this model to multiple partners, thus providing the first opportunity to test partner-specificity and generalization.
To do so, we create an additional We use a hierarchical  \cite{GelmanEtAl14_BDA,TenenbaumKempGriffithsGoodman11_Grow_a_Mind_Science,KleinschmidtJaeger15_RobustSpeechPerception} that provides a useful mathematical and conceptual framework for addressing these challenges. Hierarchical models have been key to explaining how the human mind solves difficult inductive problems in domain like causal learning \cite{KempGoodmanTenenbaum10_LearningToLearn,GoodmanUllmanTenenbaum11_TheoryOfCausality} and concept learning \cite{KempPerforsTenenbaum07_HBM} where abstract, shared properties must be jointly inferred with idiosyncratic particulars of instances. More than the relatively fixed, biological concept of a dog, though, language is a moving target. The only data we use to ground our learning is produced by other agents who are in the same position as we are, and our only goal is to coordinate on the same meanings in context \cite{HassonGhazanfar___Keysers12BrainToBrain}. This is the sense in which the meanings we learn are conventional. This \emph{social grounding} is precisely what gives rise to the fascinating idiosyncracies of local convention formation.

%Here we sketch the ``ideal'' mathematical form of the proposed model, leaving implementational details for the following section.
%Hierarchical Bayesian models have been key to quantitatively explaining how the human mind solves difficult inductive problems in domains like causal learning and concept learning where idiosyncratic particulars of instances must be jointly inferred with knowledge that is shared across instances \shortcite{tenenbaum_how_2011}.
%For instance, the concept of a ``dog'' abstracts away from our experiences with different instances of dogs across a lifetime, and provides stable expectations about the properties of a new instance -- four legs, wagging tail, barking noises.

%More than the relatively fixed, biological concept of a dog, though, language can be seen as a moving target. The only data we use to ground our learning is produced by other agents who are in the same position as we are, and our only goal is to coordinate on the same meanings in context. This is the sense in which the meanings we learn are conventional. 
However, extensive experience with a particular dog \emph{Fido} reveals idiosyncratic properties, like the pattern of spots on his coat.
This example from concept learning can be adapted as a novel perspective on how conventions work: accumulated knowledge about linguistic conventions in one's community provides stable communicative ``priors'' that guide how we approach new partners, but the language we use to talk with a family member or close collaborator may deviate considerably from the usage predicted by population-level conventions.
Hierarchical Bayesian models thus provide a formal method for both smoothly integrating population-level expectations with partner-specific ones, and also appropriately \emph{updating} population-level knowledge through additional partner-specific observations (see Fig. \ref{fig:task1model}).

Just as our concept of a dog, built up over many individual experiences across a lifetime, provides stable expectations about the properties of a new instance -- four legs, wagging tail, barking noises -- our accumulated lexical knowledge provides stable communicative expectations. 
At the highest level of the hierarchical lexical representation is a \emph{community-level} variable $\Theta$ parameterizing the agent's prior expectations for the likely lexicon $\mathcal{L}_i$ used by a novel community member $i$: $P(\mathcal{L}_i | \Theta)$. 

\begin{figure}
\centering
    \includegraphics[scale=1.1]{partnerspecificity.pdf}
  \caption{Hierarchical model}
  \label{fig:specificity}
\end{figure}

%For the conceptual purposes of this chapter, it is not important exactly what form this distribution takes, or what initial prior over the over-hypothesis $P(\Theta)$ could in principle guide early language learning
For simplicity, we assume $\Theta$ parameterizes %an $\mathcal{W} \times \mathcal{O} \times 2$ tensor containing values $(\alpha_{(w,o)}, \beta_{(w,o)})$ for every entry $(w,o)$ in the lexicon $\mathcal{L}_i$. 
This would factor the lexical prior $P(\mathcal{L}_i | \Theta)$ into independent Beta distributions over intervals $[0,1]$. It would then be straightforward to place an uninformative prior $P(\Theta)$ over that tensor which does not overwhelm the likelihood \cite<see>[p. 110, for some reasonable choices]{GelmanEtAl14_BDA}. 
More generally, we could allow for arbitrarily complex dependencies between entries of the lexicon by using a Bayesian neural network with weight tensor $\Theta$.
In Chapter 4 we will propose a neural network approximation for empirical Bayes, representing only a point estimate of $\Theta$ rather than a whole distribution.
Here, it only matters that this knowledge is hierarchical: we expect all members of our language community to share some commonality in what they mean by things. 

Now that we have defined a hierarchical likelihood on lexical beliefs, we must say how we \emph{learn} partner-specific models. 
Just as years of living with a particular dog Fido reveals more specific properties than would be expected solely a general dog concept, the language we use to talk with a family member or close collaborator in a particular context may deviate considerably from the usage predicted by global conventions. 

In other words, our partner-specific beliefs about a particular individual's semantics $\mathcal{L}_i$ are formed by integrating our abstract lexical knowledge $\Theta$ with particular  observations $D_i$ of that particular individual, concretely, utterances and responses in a reference game:
$$%\begin{array}{rcl}
P(\mathcal{L}_i | D_i)  \propto \int_{\Theta}P(\mathcal{L}_i | D_i,  \Theta) P(\Theta | D_i) 
%                     & = & \mathbb{E}_{\Theta_0}[P(\mathcal{L}_i | \Theta_0, D_i)] 
%\end{array}
$$
where the posteriors in the integral can be computed using Bayes rule:
$$
P(\mathcal{L}_i | D_i, \Theta) \propto P(D_i | \mathcal{L}_i, \Theta) P(\mathcal{L}_i | \Theta)
$$
Note that our posterior beliefs about $\Theta$ are in fact informed by observations from \emph{all} speakers: $D = \bigcup_{i=1}^k D_i$. 
Additionally, because the partner-specific model depends on $\Theta$, Bayesian inference allows new data to systematically inform the shared, population-level representation as well (Fig. \ref{fig:task1model}).
Critically for predictions about generalization, new language data (i.e. particular ways of referring to the tangram shapes) may at first be more parsimoniously explained as an idiosyncratic property of a particular partner's lexicon, or ``idiolect''. 
If two or three partners all happen to use the same language, however, it starts to become more likely that a novel partner will share it as well (this transfer is sometimes referred to as ``sharing of statistical strength.'')
This formalizes the intuition from the behavioral predictions.

%For  adult language users who have observed innumerable uses of language over their lifetimes, the contribution of a new data point to the overhypothesis $P(\Theta_0 | D)$ should be negligible; the contribution to a partner-specific model, however, can be quite strong. 

Finally, to fully specify our model and compute our partner-specific lexical posterior $P(\mathcal{L}_i, D_i, \Theta_0)$, we must link our beliefs about a partner's lexica to their actual behavior with a likelihood function $P(D_i | \mathcal{L}_i, \Theta_0)$. 
This is naturally supplied by the Rational Speech Act framework in the previous section \cite{FrankGoodman12_PragmaticReasoningLanguageGames,GoodmanFrank16_RSATiCS,BergenLevyGoodman16_LexicalUncertainty,SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS}: we assume speakers produce utterances that are parsimonious yet informative in context with respect to their lexicon, and listeners interpret utterances by inverting a speaker model. Because we expect our partner to use language rationally given some lexicon, the utterance they choose to refer to some object will be probable under some lexica and highly improbable under others. 
In this way, a particular agent's language use is a cue to their particular lexicon as well as a cue to the communal lexicon shared. 

In summary, our hierarchical model formalizes the intuition that global conventions are learned and generalized over many extended interactions with many different people across a lifetime, and that this shared semantic prototype is the backbone supporting rapid learning for new partners and situations. 


\section{Discussion}

There is a broader debate over the timescales at which lexicons and lexicon learning mechanisms operate; here, we assume a discourse-level structure to the lexicon, where there is uncertainty over how words are used \emph{in the given conversation, by the current partner}. See \cite{FrankGoodmanTenenbaum09_Wurwur} and others for a related approach at the developmental timescale of cross-situational word learning.

Theories of convention-formation vary in the extent to which social reasoning about common ground is required. 
Our agents lie on a spectrum between the heuristic updating agents of Barr (2004) and the sophisticated agents of Clark \& Wilkes-Gibbs (1986), who collaboratively build up explicit representations of mutual knowledge. 
Speakers and listeners in our model implicitly coordinate their beliefs through a shared history of observations, which serves as ``common ground'' in an informal sense.
They make critical use of pragmatic, social reasoning in order to learn meanings, but do not explicitly consider the fact that this history is shared, or represent their partner's own uncertainty.

By capturing reduction, which purely heuristic theories have not yet demonstrated, we showed that minimal assumptions of social reasoning go a long way in accounting for key phenomena. 
Still, our model falls short in some ways. 
For instance, because we do not provide a mechanisms for the listener agent to respond with confirmation, repair, or follow-up questions, we cannot make explicit predictions about the reduction in \emph{listener messages} (as in Fig. \ref{fig:replication}) or the impact of early listener responses on conventionalization. 
These phenomena require our model to deal with planning over extended dialogues, and to potentially weaken the assumption that one's partner knows the true lexicon with complete certainty. 

Similarly, while our model was explicitly designed with linguistic conventions in mind, it remains to be seen whether the same formulation generalizes to broader behavioral conventions. 
For example, the real-time coordination games used in Hawkins \& Goldstone (2016) may not require players to reason about a structured lexicon with noise, but an action policy representation may play a similar role. 
While there remain many complex aspects of convention-formation in communication games left for future research, our approach nonetheless serves as a lower bound on the degree of social reasoning needed to capture lexical conventions in these games.
